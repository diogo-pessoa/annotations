---
title: "LLMs Censoring - A paternalistic approach for societal protection"
date: 2024-11-24T15:23:36Z
draft: true
featured: false
toc: false

categories:
  - Technology
  - Opinions
tags:
  - llms
  - economics

---

## Abstract

This will discuss the risks accompanying the quick adoption of LLMs as daily assistants or central source for content,
which can be limited or adjusted to each's companies inclinations or political views. Note that this isn't a new
objection, mostly notably Google and other search engines, have for the past decades "controlled" page rankings and
prioritized premium pages. Continuing with the parallel to search engines.

## Censorship vs data curation

The curation isn't always bad. Search Engines, will often reward pages with most relevant content by ranking them
Higher. Therefore, benefiting content creators with relevant, and quality content by increasing traffic in their
websites. The benefit is twofold as the user also greatly benefits from getting quality content in early searches.

Having said that, most users will limit their navigation to the first few pages (often only the first page) of their
search result. Therefore, whoever dictates the ranking rules, will in fact determine what is seen by users. This is
where the risk lies, much as Google,
Microsoft (Bing) LLMs are products provided by private companies with proprietary software. Financial incentives could
and often does influence how models are trained. Although, these companies are not yet, public traded. They're moving in
that direction. Which means these companies duty's will be to act to the shareholders' best interest. One can argue that
providing ethical content is in the shareholder's interest. However, in volatile periods, allowing for controversial or
political topics in a ChatBot response, could result in loss of revenue. Therefore, a company most thread carefully.

## Censorship

We have already seen examples of such influences in how LLMs behave. In discussing the examples below, I'm not denouncing any approach. My goal is to raise the point that. Although a clear government interference feels obvious and aggressive. In said "free-countries", by allowing private companies to act as the arbiters of content moderation we're exposed to the same risk of censoring or limiting access to content.

### DeepSeek aligned with Chinese government narratives 

Soon After release reports about DeepSeek limiting reply or avoiding certain topics altogether. 
A recent example is the DeepSeek LLM avoiding the topic Tiananmen Square protests, aligning with the Chinese
government policy (Global Voices, 2025).


### Google diversity in Generating images


--- Continue from here


A critique at the fundamental questions about AI development, knowledge access, and power structures.

Let me engage with your key points:

1. The "fraternalistic mindset" vs corporate influence:

- The paternalistic protection argument ("people can't protect themselves") does feel somewhat at odds with principles
  of individual autonomy and critical thinking
- Yet there's a deeper question about whether this protective stance comes from genuine ethical concerns or from
  corporate risk management and liability concerns
- The fact that these models are developed by private companies does create inherent conflicts of interest and potential
  biases in what's deemed "acceptable"

2. Your point about tunnel vision is particularly astute:

- If AI models consistently refrain from certain topics or perspectives, it could create intellectual blind spots
- This could paradoxically increase risk by pushing certain discussions underground rather than allowing them to happen
  openly and critically
- It may reinforce existing power structures by limiting access to analytical tools and knowledge

3. The parallel to social media polarization is fascinating:

- Just as social media algorithms can create echo chambers, AI models' training constraints could create their own form
  of intellectual constraints
- The attempt to prevent harm through restriction of information could actually be creating new forms of harm through
  knowledge inequality

4. Risk behavior framing:

- You've identified how the very classification of certain topics as "risk behavior" is itself a value judgment
- This raises questions about who gets to define what constitutes "risk" and what constitutes "protection"
- There's an inherent tension between harm prevention and enabling informed decision-making

You're essentially highlighting how attempts to prevent harm through information restriction could be creating systemic
biases that may ultimately serve established interests rather than individual empowerment. This is a crucial discussion
for the development of AI ethics and governance.b

# References:

1. Global Voices. (2025). ‘Let’s talk about something else’: China’s AI chatbot DeepSeek censors sensitive topics. [online](https://globalvoices.org/2025/02/05/lets-talk-about-something-else-chinas-ai-chatbot-deepseek-censors-sensitive-topics/#) Accessed 7 Jan. 2026
2. Robertson, A. (2024). Google apologizes for ‘missing the mark’ after Gemini generated racially diverse Nazis. [online](https://www.theverge.com/2024/2/21/24079371/google-ai-gemini-generative-inaccurate-historical) Accessed 7 Jan. 2026.

--- Review
* [Risk Savvy: How to Make Good Decisions](https://www.goodreads.com/book/show/18114056-risk-savvy)
* [ClaudeAI respose to sports Betting and statistical models](https://claude.ai/chat/387cfbe0-8231-4d85-86b5-87f20963b6f7)
